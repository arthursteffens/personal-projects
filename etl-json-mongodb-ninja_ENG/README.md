# ETL Project
Loading data from JSON files into MongoDB.

## Details
In this project, I was presented with a dataset of over 250k JSON files. The files come from a NFT (Non-fungible tokens) project, a blockchain technology that has been talking a lot lately. The NFT project proposes a collection of more than 1000 unique characters, the ninja Johnny in various situations.

However, I used the complete combination generated by the team, which contains 256,093 different characters, one in each JSON file containing the name and characteristics. It is difficult to analyze the data with this high amount of files. So using Python I loaded all the data into a document-oriented NoSQL database, MongoDB, and I made a simple dashboard in PowerBI to filter the character by its characteristics.



## Usage
To properly run this script, you'll need Python and Docker.

- Clone this repository
- Unzip sample.7z folder
- Install required python packages: ```pip install -r requirements.txt```
- Run ```docker-compose up -d```. This will run a MongoDB instance in a docker container.
- Run the main script: ```python main.py```
  
## Obs
- The sample folder contains 1% of the total files, which means only 256 files. To use the full set, unzip ```json256k.7z``` and uncomment the line indicated in the script ```main.py```
- Instead of reading the full set of files, you can read ```full_df.csv``` directly. Instructions are inside the mentioned script.

- The dashboard made with Power BI is on [this link](https://app.powerbi.com/view?r=eyJrIjoiYjBkNTMyYmItNGRkZi00Zjk3LTljZmYtY2E4ZGIxMTU0NGFlIiwidCI6ImQ4MWM1YzY1LTM3MWYtNDg0Yi1iOTZjLWRmYzI0NGY0YzQwZiJ9&pageName=ReportSection) or in my [personal projects website](https://arthursteffens.github.io/project1_etl-json-mongodb.html).